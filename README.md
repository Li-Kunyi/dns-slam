# DNS SLAM Dense Neural Semantic-Informed SLAM

<!-- ### [Paper](https://arxiv.org/pdf/2304.14377.pdf) | [Project Page](https://hengyiwang.github.io/projects/CoSLAM) | [Video](https://hengyiwang.github.io/projects/Co-SLAM/videos/presentation.mp4) -->

> DNS SLAM Dense Neural Semantic-Informed SLAM <br />
<!-- > [Hengyi Wang](https://hengyiwang.github.io/), [Jingwen Wang](https://jingwenwang95.github.io/), [Lourdes Agapito](http://www0.cs.ucl.ac.uk/staff/L.Agapito/)<br /> -->
> IROS 2024

<!-- <p align="center">
  <a href="">
    <img src="./media/coslam_teaser.gif" alt="Logo" width="80%">
  </a>
</p> -->



This repository contains the code for the paper DNS-SLAM, a neural semantic SLAM method that perform real-time camera tracking and dense reconstruction based on a joint encoding.



## Update

- [x] Code for DNS-SLAM [2024-8-20]

## Installation
Please follow the instructions below to install the repo and dependencies.

```bash
git clone ...
cd dns-slam
```


### Install the environment

```bash
# Create conda environment
conda create -n dns-slam python=3.7
conda activate dns-slam

# Install the pytorch first (Please check the cuda version)
pip install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1 -f https://download.pytorch.org/whl/cu113/torch_stable.html

# Install all the dependencies via pip (Note here pytorch3d and tinycudann requires ~10min to build)
pip install -r requirements.txt
```



For tinycudann, if you cannot access network when you use GPUs, you can also try build from source as below:

```bash
# Build tinycudann 
git clone --recursive https://github.com/nvlabs/tiny-cuda-nn

# Try this version if you cannot use the latest version of tinycudann
#git reset --hard 91ee479d275d322a65726435040fc20b56b9c991
cd tiny-cuda-nn/bindings/torch
python setup.py install
```



## Dataset

#### Replica

Download the sequences of the Replica Dataset generated by the authors of vMAP into your dataset folder. 


#### ScanNet

Please follow the procedure on [ScanNet](http://www.scan-net.org/) website, and extract color & depth frames from the `.sens` file using the [code](https://github.com/ScanNet/ScanNet/blob/master/SensReader/python/reader.py).



## Run

You can run DNS-SLAM using the code below:

```
# replica
python run.py configs/replica/room_0.yaml

#scannet
python run.py configs/scannet/scene0000.yaml
```



## Evaluation

You can run trajectory evaluation using the code below:
```
# replica
python eval_ate.py configs/replica/office_0.yaml

#scannet
python eval_ate.py configs/scannet/scene0000.yaml
```

You can run reconstruction evaluation using the code below:
```
# replica

#scannet

```


You can run visulation using the code below:
```
# replica
python visualizer.py configs/replica/office_0.yaml

#scannet
python visualizer.py configs/scannet/scene0000.yaml
```


## Acknowledgement

<!-- We adapt codes from some awesome repositories, including [NICE-SLAM](https://github.com/cvg/nice-slam), [NeuralRGBD](https://github.com/dazinovic/neural-rgbd-surface-reconstruction), [tiny-cuda-nn](https://github.com/NVlabs/tiny-cuda-nn). Thanks for making the code available. We also thank [Zihan Zhu](https://zzh2000.github.io/) of [NICE-SLAM](https://github.com/cvg/nice-slam), [Edgar Sucar](https://edgarsucar.github.io/) of [iMAP](https://edgarsucar.github.io/iMAP/) for their prompt responses to our inquiries regarding the details of their methods. 

The research presented here has been supported by a sponsored research award from Cisco Research and the UCL Centre for Doctoral Training in Foundational AI under UKRI grant number EP/S021566/1. This project made use of time on Tier 2 HPC facility JADE2, funded by EPSRC (EP/T022205/1). -->



## Citation

If you find our code or paper useful for your research, please consider citing:

